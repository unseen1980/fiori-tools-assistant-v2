{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFpWROGSyd6p"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n",
        "\n",
        "[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n",
        "[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Finetuning Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "</div>\n",
        "\n",
        "ðŸ‘‹ Welcome to Open Universal Machine Intelligence (Oumi)!\n",
        "\n",
        "ðŸš€ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n",
        "\n",
        "ðŸ¤ Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n",
        "\n",
        "â­ If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFJMDj8Uyd6s"
      },
      "source": [
        "# Finetuning Overview\n",
        "\n",
        "In this tutorial, we'll LoRA tune a large language model to produce \"thoughts\" before producing its output.\n",
        "\n",
        "We'll use the Oumi framework to streamline the process and achieve high-quality results.\n",
        "\n",
        "We'll cover the following topics:\n",
        "1. Prerequisites\n",
        "2. Data Preparation & Sanity Checks\n",
        "3. Training Config Preparation\n",
        "4. Launching Training\n",
        "5. Monitoring Progress\n",
        "6. Evaluation\n",
        "7. Analyzing Results\n",
        "8. Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EImkGKVGyd6s"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "â—**NOTICE:** We recommend running this notebook on a GPU. If running on Google Colab, you can use the free T4 GPU runtime (Colab Menu: `Runtime` -> `Change runtime type`). On Colab, we recommend replacing `HuggingFaceTB/SmolLM2-1.7B-Instruct` with a smaller model like `HuggingFaceTB/SmolLM2-135M-Instruct`, since the T4 only has 16GB VRAM; you can use `Edit -> Find and replace` in the menu bar to do so.\n",
        "\n",
        "First, let's install Oumi. You can find more detailed instructions [here](https://oumi.ai/docs/en/latest/get_started/installation.html). Here, we include Oumi's GPU dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5TkKAL9gyd6s",
        "outputId": "f684d7c4-16b4-4ab7-8c73-71457645bcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m\n",
            "Collecting pillow<10\n",
            "  Using cached Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Using cached Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 10.3.0\n",
            "    Uninstalling pillow-10.3.0:\n",
            "      Successfully uninstalled pillow-10.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mistral-common 1.5.3 requires pillow>=10.3.0, but you have pillow 9.5.0 which is incompatible.\n",
            "oumi 0.1.3 requires pillow<10.4,>=10.3.0, but you have pillow 9.5.0 which is incompatible.\n",
            "scikit-image 0.25.1 requires pillow>=10.1, but you have pillow 9.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-9.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "4d6db19656ae4a70929f7b796f8046f4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install uv -q\n",
        "!uv pip install oumi[gpu] vllm --no-progress --system\n",
        "!pip install --upgrade \"pillow<10\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR0VvB0Ryd6t"
      },
      "source": [
        "## Creating our working directory\n",
        "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QsaZrVj7yd6t"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "tutorial_dir = \"finetuning_tutorial\"\n",
        "\n",
        "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm0C9FuIyd6t"
      },
      "source": [
        "## Setup the environment\n",
        "\n",
        "You may need to set the following environment variables:\n",
        "- [Optional] HF_TOKEN: Your [HuggingFace](https://huggingface.co/docs/hub/en/security-tokens) token, in case you want to access a private model like Llama.\n",
        "- [Optional] WANDB_API_KEY: Your [wandb](https://wandb.ai) token, in case you want to log your experiments to wandb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAqDbNXVyd6u"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "\n",
        "## Data Preparation\n",
        "Let's start by checking out our datasets, and seeing what the data looks like. The OpenO1-SFT dataset includes a variety of tasks, including code generation and explanation, with most examples having a \"thought\" produced prior to the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ypEM0ncHyd6u",
        "outputId": "75fbd5ca-40c8-4519-fe78-0a9bf0008558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-16 22:13:11,231][oumi][rank0][pid:7702][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: PromptResponseDataset) dataset_name: 'Unseen1980/fiori-tools-support-ga', dataset_path: 'None'...\n",
            "[2025-02-16 22:13:12,117][oumi][rank0][pid:7702][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 297339\n",
            "\tDownload size: 296628\n",
            "\tSize: 593967 bytes\n",
            "\tRows: 417\n",
            "\tColumns: ['question', 'answer']\n",
            "[2025-02-16 22:13:12,450][oumi][rank0][pid:7702][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (417, 2). Columns:\n",
            "question    object\n",
            "answer      object\n",
            "dtype: object\n",
            "Example 1:\n",
            "user: Why is cmd+space not triggering code completion on my Mac?...\n",
            "assistant: The shortcut for triggering code completion cmd+space does not work on Mac. This shortcut is used fo...\n",
            "\n",
            "\n",
            "Example 2:\n",
            "user: I need help with code completion not activating using cmd+space on Mac....\n",
            "assistant: The shortcut for triggering code completion cmd+space does not work on Mac. This shortcut is used fo...\n",
            "\n",
            "\n",
            "Example 3:\n",
            "user: My cmd+space shortcut is malfunctioning for code suggestions in VS Code on Mac. What should I do?...\n",
            "assistant: The shortcut for triggering code completion cmd+space does not work on Mac. This shortcut is used fo...\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from oumi.builders import build_tokenizer\n",
        "from oumi.core.configs import ModelParams\n",
        "from oumi.datasets import PromptResponseDataset\n",
        "\n",
        "# Initialize the dataset\n",
        "tokenizer = build_tokenizer(\n",
        "    ModelParams(model_name=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
        ")\n",
        "dataset = PromptResponseDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    hf_dataset_path=\"Unseen1980/fiori-tools-support-ga\",\n",
        "    prompt_column=\"question\",\n",
        "    response_column=\"answer\",\n",
        ")\n",
        "\n",
        "# Print a few examples\n",
        "for i in range(3):\n",
        "    conversation = dataset.conversation(i)\n",
        "    print(f\"Example {i + 1}:\")\n",
        "    for message in conversation.messages:\n",
        "        print(f\"{message.role}: {message.content[:100]}...\")  # Truncate for brevity\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hPodK0oyd6u"
      },
      "source": [
        "## Model Preparation\n",
        "\n",
        "For code generation, we want a model with strong general language understanding and coding capabilities.\n",
        "\n",
        "We also want a model that is small enough to train and run on a single GPU.\n",
        "\n",
        "Some good options include:\n",
        "- [\"microsoft/Phi-3-mini-128k-instruct\"](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)\n",
        "- [\"google/gemma-2b\"](https://huggingface.co/google/gemma-2b)\n",
        "- [\"Qwen/Qwen2-1.5B-Instruct\"](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)\n",
        "- [\"meta-llama/Llama-3.2-3B-Instruct\"](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
        "- [\"HuggingFaceTB/SmolLM2-1.7B-Instruct\"](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)\n",
        "\n",
        "\n",
        "For this tutorial, we'll use \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" as our base model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj4uwMDIyd6u"
      },
      "source": [
        "## Initial Model Responses\n",
        "\n",
        "Let's see how our model performs on an example prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "umETxbyDyd6v",
        "outputId": "c4ffa1d7-4ab1-4c66-f2a8-753c468a5009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting finetuning_tutorial/infer.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/infer.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 128\n",
        "  batch_size: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lIgxnMJByd6v",
        "outputId": "5d5ca2c2-c686-4935-d0f3-908ecf449fec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-16 22:17:14,089][oumi][rank0][pid:7702][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-02-16 22:17:14,090][oumi][rank0][pid:7702][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-16 22:17:14,091][oumi][rank0][pid:7702][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-16 22:17:16,010][oumi][rank0][pid:7702][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n",
            "conversation_id=None messages=[USER: Can you help me with an issue where my SAP Fiori preview won't load?, ASSISTANT: Of course, I'd be happy to help you troubleshoot the issue with your SAP Fiori preview. Here are some steps you can take:\n",
            "\n",
            "1. **Check your browser**: Make sure you're using a supported browser like Chrome, Firefox, or Edge. SAP Fiori is not compatible with Internet Explorer.\n",
            "\n",
            "2. **Update your browser**: Ensure your browser is up-to-date. Outdated browsers can cause compatibility issues.\n",
            "\n",
            "3. **Clear your browser cache**: Sometimes, old or corrupted cache files can cause issues. Clearing your browser cache can help.\n",
            "\n",
            "4. **Disable extensions] metadata={}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.infer import infer\n",
        "\n",
        "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"infer.yaml\"))\n",
        "\n",
        "input_text = (\n",
        "    \"Can you help me with an issue where my SAP Fiori preview won't load?\"\n",
        ")\n",
        "\n",
        "results = infer(config=config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLL-Ygoyd6v"
      },
      "source": [
        "## Preparing our training experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XLFmf5Tyd6v"
      },
      "source": [
        "Let's create a YAML file for our training config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zPJ8UU8lyd6v",
        "outputId": "38989f20-6e3e-48d0-befa-47dc425a3295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting finetuning_tutorial/train.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/train.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  tokenizer_pad_token: \"<|endoftext|>\"\n",
        "  device_map: \"auto\"\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"PromptResponseDataset\"\n",
        "        split: \"train\"\n",
        "        sample_count: 8000\n",
        "        dataset_kwargs: {\n",
        "          \"hf_dataset_path\": \"Unseen1980/fiori-tools-support-ga\",\n",
        "          \"prompt_column\": \"question\",\n",
        "          \"response_column\": \"answer\",\n",
        "          \"assistant_only\": true,\n",
        "          \"instruction_template\": \"<|im_start|>user\\n\",\n",
        "          \"response_template\": \"<|im_start|>assistant\\n\",\n",
        "        }\n",
        "        shuffle: True\n",
        "        seed: 42\n",
        "    collator_name: \"text_with_padding\"\n",
        "    seed: 42\n",
        "\n",
        "training:\n",
        "  output_dir: \"finetuning_tutorial/output\"\n",
        "\n",
        "  # For a single GPU, the following gives us a batch size of 16\n",
        "  # If training with multiple GPUs, feel free to reduce gradient_accumulation_steps\n",
        "  per_device_train_batch_size: 2\n",
        "  gradient_accumulation_steps: 8\n",
        "\n",
        "  # ***NOTE***\n",
        "  # We set it to 10 steps to first verify that it works\n",
        "  # Swap to 1500 steps to get more meaningful results.\n",
        "  # Note: 1500 steps will take 2-3 hours on a single A100-40GB GPU.\n",
        "  # max_steps: 10\n",
        "  max_steps: 1500\n",
        "\n",
        "  learning_rate: 1e-3\n",
        "  warmup_ratio: 0.1\n",
        "  logging_steps: 10\n",
        "  save_steps: 0\n",
        "  max_grad_norm: 1\n",
        "  weight_decay: 0.01\n",
        "\n",
        "\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  optimizer: \"adamw_torch_fused\"\n",
        "  enable_gradient_checkpointing: True\n",
        "  gradient_checkpointing_kwargs:\n",
        "    use_reentrant: False\n",
        "  ddp_find_unused_parameters: False\n",
        "  dataloader_num_workers: \"auto\"\n",
        "  dataloader_prefetch_factor: 32\n",
        "  empty_device_cache_steps: 1\n",
        "  use_peft: true\n",
        "\n",
        "peft:\n",
        "  lora_r: 16\n",
        "  lora_alpha: 32\n",
        "  lora_dropout: 0.00\n",
        "  lora_target_modules:\n",
        "    - \"q_proj\"\n",
        "    - \"k_proj\"\n",
        "    - \"v_proj\"\n",
        "    - \"o_proj\"\n",
        "    - \"gate_proj\"\n",
        "    - \"up_proj\"\n",
        "    - \"down_proj\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UILbPHbyd6v"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "eJ9HdwMmyd6v"
      },
      "source": [
        "This will start the fine-tuning process using the Oumi framework. Because we set `max_steps: 5`, this should be very quick. The full fine-tuning process may take a few hours, depending on your GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4X1l5_Syd6v"
      },
      "source": [
        "### SINGLE GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "GiOyl_iUyd6v",
        "outputId": "3fbdb225-180a-490b-b620-ac42afc4963c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "[2025-02-16 22:24:35,649][oumi][rank0][pid:11003][MainThread][INFO]][distributed.py:546] Setting random seed to 42 on rank 0.\n",
            "2025-02-16 22:24:37.425780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739744677.448051   11003 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739744677.454876   11003 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-02-16 22:24:40,033][oumi][rank0][pid:11003][MainThread][INFO]][torch_utils.py:66] Torch version: 2.4.0+cu121. NumPy version: 1.26.4\n",
            "[2025-02-16 22:24:40,034][oumi][rank0][pid:11003][MainThread][INFO]][torch_utils.py:72] CUDA version: 12.1 CuDNN version: 90.1.0\n",
            "[2025-02-16 22:24:40,044][oumi][rank0][pid:11003][MainThread][INFO]][torch_utils.py:106] CPU cores: 12 CUDA devices: 1\n",
            "device(0)='NVIDIA A100-SXM4-40GB' Capability: (8, 0) Memory: [Total: 39.56GiB Free: 33.06GiB Allocated: 0.0GiB Cached: 0.0GiB]\n",
            "[2025-02-16 22:24:40,046][oumi][rank0][pid:11003][MainThread][INFO]][train.py:133] Oumi version: 0.1.3\n",
            "[2025-02-16 22:24:40,048][oumi][rank0][pid:11003][MainThread][INFO]][train.py:144] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'\n",
            "[2025-02-16 22:24:40,050][oumi][rank0][pid:11003][MainThread][INFO]][train.py:174] TrainingConfig:\n",
            "TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='PromptResponseDataset',\n",
            "                                                                                dataset_path=None,\n",
            "                                                                                subset=None,\n",
            "                                                                                split='train',\n",
            "                                                                                dataset_kwargs={'assistant_only': True,\n",
            "                                                                                                'hf_dataset_path': 'Unseen1980/fiori-tools-support-ga',\n",
            "                                                                                                'instruction_template': '<|im_start|>user\\n',\n",
            "                                                                                                'prompt_column': 'question',\n",
            "                                                                                                'response_column': 'answer',\n",
            "                                                                                                'response_template': '<|im_start|>assistant\\n'},\n",
            "                                                                                sample_count=8000,\n",
            "                                                                                mixture_proportion=None,\n",
            "                                                                                shuffle=True,\n",
            "                                                                                seed=42,\n",
            "                                                                                shuffle_buffer_size=1000,\n",
            "                                                                                trust_remote_code=False,\n",
            "                                                                                transform_num_workers=None)],\n",
            "                                                        collator_name='text_with_padding',\n",
            "                                                        pack=False,\n",
            "                                                        stream=False,\n",
            "                                                        target_col=None,\n",
            "                                                        mixture_strategy='first_exhausted',\n",
            "                                                        seed=42,\n",
            "                                                        use_async_dataset=False,\n",
            "                                                        use_torchdata=None),\n",
            "                               test=DatasetSplitParams(datasets=[],\n",
            "                                                       collator_name=None,\n",
            "                                                       pack=False,\n",
            "                                                       stream=False,\n",
            "                                                       target_col=None,\n",
            "                                                       mixture_strategy='first_exhausted',\n",
            "                                                       seed=None,\n",
            "                                                       use_async_dataset=False,\n",
            "                                                       use_torchdata=None),\n",
            "                               validation=DatasetSplitParams(datasets=[],\n",
            "                                                             collator_name=None,\n",
            "                                                             pack=False,\n",
            "                                                             stream=False,\n",
            "                                                             target_col=None,\n",
            "                                                             mixture_strategy='first_exhausted',\n",
            "                                                             seed=None,\n",
            "                                                             use_async_dataset=False,\n",
            "                                                             use_torchdata=None)),\n",
            "               model=ModelParams(model_name='HuggingFaceTB/SmolLM2-1.7B-Instruct',\n",
            "                                 adapter_model=None,\n",
            "                                 tokenizer_name=None,\n",
            "                                 tokenizer_pad_token='<|endoftext|>',\n",
            "                                 tokenizer_kwargs={},\n",
            "                                 model_max_length=None,\n",
            "                                 load_pretrained_weights=True,\n",
            "                                 trust_remote_code=True,\n",
            "                                 torch_dtype_str='bfloat16',\n",
            "                                 compile=False,\n",
            "                                 chat_template=None,\n",
            "                                 attn_implementation=None,\n",
            "                                 device_map='auto',\n",
            "                                 model_kwargs={},\n",
            "                                 enable_liger_kernel=False,\n",
            "                                 shard_for_eval=False,\n",
            "                                 freeze_layers=[]),\n",
            "               training=TrainingParams(use_peft=True,\n",
            "                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,\n",
            "                                       enable_gradient_checkpointing=True,\n",
            "                                       gradient_checkpointing_kwargs={'use_reentrant': False},\n",
            "                                       output_dir='finetuning_tutorial/output',\n",
            "                                       per_device_train_batch_size=2,\n",
            "                                       per_device_eval_batch_size=8,\n",
            "                                       gradient_accumulation_steps=8,\n",
            "                                       max_steps=1500,\n",
            "                                       num_train_epochs=3,\n",
            "                                       save_epoch=False,\n",
            "                                       save_steps=0,\n",
            "                                       save_final_model=True,\n",
            "                                       seed=42,\n",
            "                                       run_name=None,\n",
            "                                       metrics_function=None,\n",
            "                                       log_level='info',\n",
            "                                       dep_log_level='warning',\n",
            "                                       enable_wandb=False,\n",
            "                                       enable_tensorboard=True,\n",
            "                                       logging_strategy='steps',\n",
            "                                       logging_dir=None,\n",
            "                                       logging_steps=10,\n",
            "                                       logging_first_step=False,\n",
            "                                       eval_strategy='no',\n",
            "                                       eval_steps=500,\n",
            "                                       learning_rate=0.001,\n",
            "                                       lr_scheduler_type='linear',\n",
            "                                       lr_scheduler_kwargs={},\n",
            "                                       warmup_ratio=0.1,\n",
            "                                       warmup_steps=None,\n",
            "                                       optimizer='adamw_torch_fused',\n",
            "                                       weight_decay=0.01,\n",
            "                                       adam_beta1=0.9,\n",
            "                                       adam_beta2=0.999,\n",
            "                                       adam_epsilon=1e-08,\n",
            "                                       sgd_momentum=0.0,\n",
            "                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,\n",
            "                                       compile=False,\n",
            "                                       include_performance_metrics=False,\n",
            "                                       include_alternative_mfu_metrics=False,\n",
            "                                       log_model_summary=False,\n",
            "                                       resume_from_checkpoint=None,\n",
            "                                       try_resume_from_last_checkpoint=False,\n",
            "                                       dataloader_num_workers=2,\n",
            "                                       dataloader_prefetch_factor=32,\n",
            "                                       dataloader_main_process_only=None,\n",
            "                                       ddp_find_unused_parameters=False,\n",
            "                                       max_grad_norm=1.0,\n",
            "                                       trainer_kwargs={},\n",
            "                                       profiler=ProfilerParams(save_dir=None,\n",
            "                                                               enable_cpu_profiling=False,\n",
            "                                                               enable_cuda_profiling=False,\n",
            "                                                               record_shapes=False,\n",
            "                                                               profile_memory=False,\n",
            "                                                               with_stack=False,\n",
            "                                                               with_flops=False,\n",
            "                                                               with_modules=False,\n",
            "                                                               row_limit=50,\n",
            "                                                               schedule=ProfilerScheduleParams(enable_schedule=False,\n",
            "                                                                                               wait=0,\n",
            "                                                                                               warmup=1,\n",
            "                                                                                               active=3,\n",
            "                                                                                               repeat=1,\n",
            "                                                                                               skip_first=1)),\n",
            "                                       telemetry=TelemetryParams(telemetry_dir='telemetry',\n",
            "                                                                 collect_telemetry_for_all_ranks=False,\n",
            "                                                                 track_gpu_temperature=False),\n",
            "                                       empty_device_cache_steps=1,\n",
            "                                       nccl_default_timeout_minutes=None),\n",
            "               peft=PeftParams(lora_r=16,\n",
            "                               lora_alpha=32,\n",
            "                               lora_dropout=0.0,\n",
            "                               lora_target_modules=['q_proj',\n",
            "                                                    'k_proj',\n",
            "                                                    'v_proj',\n",
            "                                                    'o_proj',\n",
            "                                                    'gate_proj',\n",
            "                                                    'up_proj',\n",
            "                                                    'down_proj'],\n",
            "                               lora_modules_to_save=None,\n",
            "                               lora_bias='none',\n",
            "                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,\n",
            "                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,\n",
            "                               q_lora=False,\n",
            "                               q_lora_bits=4,\n",
            "                               bnb_4bit_quant_type='fp4',\n",
            "                               use_bnb_nested_quant=False,\n",
            "                               bnb_4bit_quant_storage='uint8',\n",
            "                               bnb_4bit_compute_dtype='float32',\n",
            "                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),\n",
            "               fsdp=FSDPParams(enable_fsdp=False,\n",
            "                               sharding_strategy=<ShardingStrategy.FULL_SHARD: 'FULL_SHARD'>,\n",
            "                               cpu_offload=False,\n",
            "                               mixed_precision=None,\n",
            "                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,\n",
            "                               forward_prefetch=False,\n",
            "                               use_orig_params=None,\n",
            "                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,\n",
            "                               auto_wrap_policy=<AutoWrapPolicy.NO_WRAP: 'NO_WRAP'>,\n",
            "                               min_num_params=100000,\n",
            "                               transformer_layer_cls=None,\n",
            "                               sync_module_states=True))\n",
            "[2025-02-16 22:24:40,286][oumi][rank0][pid:11003][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-16 22:24:40,286][oumi][rank0][pid:11003][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "[2025-02-16 22:24:41,762][oumi][rank0][pid:11003][MainThread][INFO]][train.py:226] Building PEFT model...\n",
            "[2025-02-16 22:24:42,185][oumi][rank0][pid:11003][MainThread][INFO]][base_map_dataset.py:68] Creating map dataset (type: PromptResponseDataset) dataset_name: 'Unseen1980/fiori-tools-support-ga', dataset_path: 'None'...\n",
            "[2025-02-16 22:24:42,185][oumi][rank0][pid:11003][MainThread][WARNING]][base_sft_dataset.py:251] Response template '<|im_start|>assistant\n",
            "' contains leading or trailing whitespaces. These will be ignored.\n",
            "[2025-02-16 22:24:42,185][oumi][rank0][pid:11003][MainThread][WARNING]][base_sft_dataset.py:267] Instruction template '<|im_start|>user\n",
            "' contains leading or trailing whitespaces. These will be ignored.\n",
            "[2025-02-16 22:24:44,195][oumi][rank0][pid:11003][MainThread][INFO]][base_map_dataset.py:472] Dataset Info:\n",
            "\tSplit: train\n",
            "\tVersion: 0.0.0\n",
            "\tDataset size: 297339\n",
            "\tDownload size: 296628\n",
            "\tSize: 593967 bytes\n",
            "\tRows: 417\n",
            "\tColumns: ['question', 'answer']\n",
            "[2025-02-16 22:24:44,486][oumi][rank0][pid:11003][MainThread][INFO]][base_map_dataset.py:411] Loaded DataFrame with shape: (417, 2). Columns:\n",
            "question    object\n",
            "answer      object\n",
            "dtype: object\n",
            "[2025-02-16 22:24:44,558][oumi][rank0][pid:11003][MainThread][INFO]][base_map_dataset.py:297] PromptResponseDataset: features=dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "[2025-02-16 22:24:44,593][oumi][rank0][pid:11003][MainThread][INFO]][base_map_dataset.py:361] Finished transforming dataset (PromptResponseDataset)! Speed: 12219.23 examples/sec. Examples: 417. Duration: 0.0 sec. Transform workers: 1.\n",
            "[2025-02-16 22:24:44,935][oumi][rank0][pid:11003][MainThread][INFO]][torch_profiler_utils.py:150] PROF: Torch Profiler disabled!\n",
            "[2025-02-16 22:24:44,955][oumi][rank0][pid:11003][MainThread][INFO]][training.py:49] SFTConfig(output_dir='finetuning_tutorial/output',\n",
            "          overwrite_output_dir=False,\n",
            "          do_train=False,\n",
            "          do_eval=False,\n",
            "          do_predict=False,\n",
            "          eval_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          prediction_loss_only=False,\n",
            "          per_device_train_batch_size=2,\n",
            "          per_device_eval_batch_size=8,\n",
            "          per_gpu_train_batch_size=None,\n",
            "          per_gpu_eval_batch_size=None,\n",
            "          gradient_accumulation_steps=8,\n",
            "          eval_accumulation_steps=None,\n",
            "          eval_delay=0,\n",
            "          torch_empty_cache_steps=1,\n",
            "          learning_rate=0.001,\n",
            "          weight_decay=0.01,\n",
            "          adam_beta1=0.9,\n",
            "          adam_beta2=0.999,\n",
            "          adam_epsilon=1e-08,\n",
            "          max_grad_norm=1.0,\n",
            "          num_train_epochs=3,\n",
            "          max_steps=1500,\n",
            "          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,\n",
            "          lr_scheduler_kwargs={},\n",
            "          warmup_ratio=0.1,\n",
            "          warmup_steps=0,\n",
            "          log_level='warning',\n",
            "          log_level_replica='warning',\n",
            "          log_on_each_node=True,\n",
            "          logging_dir='finetuning_tutorial/output/runs/Feb16_22-24-44_e02d7b0587af',\n",
            "          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,\n",
            "          logging_first_step=False,\n",
            "          logging_steps=10,\n",
            "          logging_nan_inf_filter=True,\n",
            "          save_strategy=<IntervalStrategy.NO: 'no'>,\n",
            "          save_steps=0,\n",
            "          save_total_limit=None,\n",
            "          save_safetensors=True,\n",
            "          save_on_each_node=False,\n",
            "          save_only_model=False,\n",
            "          restore_callback_states_from_checkpoint=False,\n",
            "          no_cuda=False,\n",
            "          use_cpu=False,\n",
            "          use_mps_device=False,\n",
            "          seed=42,\n",
            "          data_seed=None,\n",
            "          jit_mode_eval=False,\n",
            "          use_ipex=False,\n",
            "          bf16=False,\n",
            "          fp16=False,\n",
            "          fp16_opt_level='O1',\n",
            "          half_precision_backend='auto',\n",
            "          bf16_full_eval=False,\n",
            "          fp16_full_eval=False,\n",
            "          tf32=None,\n",
            "          local_rank=0,\n",
            "          ddp_backend=None,\n",
            "          tpu_num_cores=None,\n",
            "          tpu_metrics_debug=False,\n",
            "          debug=[],\n",
            "          dataloader_drop_last=False,\n",
            "          eval_steps=500,\n",
            "          dataloader_num_workers=2,\n",
            "          dataloader_prefetch_factor=32,\n",
            "          past_index=-1,\n",
            "          run_name='finetuning_tutorial/output',\n",
            "          disable_tqdm=False,\n",
            "          remove_unused_columns=True,\n",
            "          label_names=None,\n",
            "          load_best_model_at_end=False,\n",
            "          metric_for_best_model=None,\n",
            "          greater_is_better=None,\n",
            "          ignore_data_skip=False,\n",
            "          fsdp=[],\n",
            "          fsdp_min_num_params=0,\n",
            "          fsdp_config={'min_num_params': 0,\n",
            "                       'xla': False,\n",
            "                       'xla_fsdp_grad_ckpt': False,\n",
            "                       'xla_fsdp_v2': False},\n",
            "          fsdp_transformer_layer_cls_to_wrap=None,\n",
            "          accelerator_config=AcceleratorConfig(split_batches=False,\n",
            "                                               dispatch_batches=None,\n",
            "                                               even_batches=True,\n",
            "                                               use_seedable_sampler=True,\n",
            "                                               non_blocking=False,\n",
            "                                               gradient_accumulation_kwargs=None,\n",
            "                                               use_configured_state=False),\n",
            "          deepspeed=None,\n",
            "          label_smoothing_factor=0.0,\n",
            "          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,\n",
            "          optim_args=None,\n",
            "          adafactor=False,\n",
            "          group_by_length=False,\n",
            "          length_column_name='length',\n",
            "          report_to=['tensorboard'],\n",
            "          ddp_find_unused_parameters=False,\n",
            "          ddp_bucket_cap_mb=None,\n",
            "          ddp_broadcast_buffers=None,\n",
            "          dataloader_pin_memory=True,\n",
            "          dataloader_persistent_workers=False,\n",
            "          skip_memory_metrics=True,\n",
            "          use_legacy_prediction_loop=False,\n",
            "          push_to_hub=False,\n",
            "          resume_from_checkpoint=None,\n",
            "          hub_model_id=None,\n",
            "          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "          hub_token=None,\n",
            "          hub_private_repo=False,\n",
            "          hub_always_push=False,\n",
            "          gradient_checkpointing=True,\n",
            "          gradient_checkpointing_kwargs={'use_reentrant': False},\n",
            "          include_inputs_for_metrics=False,\n",
            "          eval_do_concat_batches=True,\n",
            "          fp16_backend='auto',\n",
            "          evaluation_strategy=None,\n",
            "          push_to_hub_model_id=None,\n",
            "          push_to_hub_organization=None,\n",
            "          push_to_hub_token=None,\n",
            "          mp_parameters='',\n",
            "          auto_find_batch_size=False,\n",
            "          full_determinism=False,\n",
            "          torchdynamo=None,\n",
            "          ray_scope='last',\n",
            "          ddp_timeout=1800,\n",
            "          torch_compile=False,\n",
            "          torch_compile_backend=None,\n",
            "          torch_compile_mode=None,\n",
            "          dispatch_batches=None,\n",
            "          split_batches=None,\n",
            "          include_tokens_per_second=False,\n",
            "          include_num_input_tokens_seen=False,\n",
            "          neftune_noise_alpha=None,\n",
            "          optim_target_modules=None,\n",
            "          batch_eval_metrics=False,\n",
            "          eval_on_start=False,\n",
            "          use_liger_kernel=False,\n",
            "          eval_use_gather_object=False,\n",
            "          dataset_text_field=None,\n",
            "          packing=False,\n",
            "          max_seq_length=None,\n",
            "          dataset_num_proc=None,\n",
            "          dataset_batch_size=1000,\n",
            "          model_init_kwargs=None,\n",
            "          dataset_kwargs=None,\n",
            "          eval_packing=None,\n",
            "          num_of_sequences=1024,\n",
            "          chars_per_token=3.6,\n",
            "          use_liger=False)\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "[2025-02-16 22:24:44,974][oumi][rank0][pid:11003][MainThread][INFO]][device_utils.py:283] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=10705.0, temperature=33, fan_speed=None, fan_speeds=None, power_usage_watts=52.499, power_limit_watts=400.0, gpu_utilization=0, memory_utilization=0, performance_state=0, clock_speed_graphics=1095, clock_speed_sm=1095, clock_speed_memory=1215).\n",
            "[2025-02-16 22:24:44,974][oumi][rank0][pid:11003][MainThread][INFO]][train.py:312] Training init time: 4.941s\n",
            "[2025-02-16 22:24:44,974][oumi][rank0][pid:11003][MainThread][INFO]][train.py:313] Starting training... (TrainerType.TRL_SFT, transformers: 4.45.2)\n",
            "  0% 0/1500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
            "{'loss': 2.6813, 'grad_norm': 0.3994033634662628, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 2.4332, 'grad_norm': 0.5745214819908142, 'learning_rate': 0.00013333333333333334, 'epoch': 0.04}\n",
            "{'loss': 1.968, 'grad_norm': 0.2519703507423401, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
            "{'loss': 1.6927, 'grad_norm': 0.3983862102031708, 'learning_rate': 0.0002666666666666667, 'epoch': 0.08}\n",
            "{'loss': 1.4248, 'grad_norm': 0.5371735095977783, 'learning_rate': 0.0003333333333333333, 'epoch': 0.1}\n",
            "{'loss': 1.0786, 'grad_norm': 0.47104665637016296, 'learning_rate': 0.0004, 'epoch': 0.12}\n",
            "{'loss': 0.7388, 'grad_norm': 0.631367027759552, 'learning_rate': 0.00046666666666666666, 'epoch': 0.14}\n",
            "{'loss': 0.3808, 'grad_norm': 0.44469115138053894, 'learning_rate': 0.0005333333333333334, 'epoch': 0.16}\n",
            "{'loss': 0.2071, 'grad_norm': 0.5264113545417786, 'learning_rate': 0.0006, 'epoch': 0.18}\n",
            "{'loss': 0.1469, 'grad_norm': 0.31895503401756287, 'learning_rate': 0.0006666666666666666, 'epoch': 0.2}\n",
            "{'loss': 0.0762, 'grad_norm': 0.22294603288173676, 'learning_rate': 0.0007333333333333333, 'epoch': 0.22}\n",
            "{'loss': 0.0556, 'grad_norm': 0.33168140053749084, 'learning_rate': 0.0008, 'epoch': 0.24}\n",
            "{'loss': 0.0357, 'grad_norm': 0.2660149037837982, 'learning_rate': 0.0008666666666666667, 'epoch': 0.26}\n",
            "{'loss': 0.0223, 'grad_norm': 0.14539045095443726, 'learning_rate': 0.0009333333333333333, 'epoch': 0.28}\n",
            "{'loss': 0.0227, 'grad_norm': 0.5406046509742737, 'learning_rate': 0.001, 'epoch': 0.3}\n",
            "{'loss': 0.0235, 'grad_norm': 0.25506556034088135, 'learning_rate': 0.0009925925925925927, 'epoch': 0.32}\n",
            "{'loss': 0.0213, 'grad_norm': 0.29443931579589844, 'learning_rate': 0.000985185185185185, 'epoch': 0.34}\n",
            "{'loss': 0.0322, 'grad_norm': 0.28570812940597534, 'learning_rate': 0.0009777777777777777, 'epoch': 0.36}\n",
            "{'loss': 0.0198, 'grad_norm': 0.11535202711820602, 'learning_rate': 0.0009703703703703704, 'epoch': 0.38}\n",
            "{'loss': 0.0243, 'grad_norm': 0.45646724104881287, 'learning_rate': 0.0009629629629629629, 'epoch': 0.4}\n",
            "{'loss': 0.0171, 'grad_norm': 0.12831705808639526, 'learning_rate': 0.0009555555555555556, 'epoch': 0.42}\n",
            "{'loss': 0.0181, 'grad_norm': 0.12929050624370575, 'learning_rate': 0.0009481481481481482, 'epoch': 0.44}\n",
            "{'loss': 0.0128, 'grad_norm': 0.1114925816655159, 'learning_rate': 0.0009407407407407408, 'epoch': 0.46}\n",
            "{'loss': 0.0076, 'grad_norm': 0.048364605754613876, 'learning_rate': 0.0009333333333333333, 'epoch': 0.48}\n",
            "{'loss': 0.0049, 'grad_norm': 0.03900817781686783, 'learning_rate': 0.000925925925925926, 'epoch': 0.5}\n",
            "{'loss': 0.0036, 'grad_norm': 0.10708998888731003, 'learning_rate': 0.0009185185185185185, 'epoch': 0.52}\n",
            "{'loss': 0.0023, 'grad_norm': 0.019001519307494164, 'learning_rate': 0.0009111111111111111, 'epoch': 0.54}\n",
            "{'loss': 0.0021, 'grad_norm': 0.007014590781182051, 'learning_rate': 0.0009037037037037037, 'epoch': 0.56}\n",
            "{'loss': 0.0032, 'grad_norm': 0.0214161965996027, 'learning_rate': 0.0008962962962962963, 'epoch': 0.58}\n",
            "{'loss': 0.0073, 'grad_norm': 0.1188969761133194, 'learning_rate': 0.0008888888888888888, 'epoch': 0.6}\n",
            "{'loss': 0.003, 'grad_norm': 0.031337589025497437, 'learning_rate': 0.0008814814814814816, 'epoch': 0.62}\n",
            "{'loss': 0.0022, 'grad_norm': 0.0074232215993106365, 'learning_rate': 0.0008740740740740741, 'epoch': 0.64}\n",
            "{'loss': 0.0032, 'grad_norm': 0.03571828827261925, 'learning_rate': 0.0008666666666666667, 'epoch': 0.66}\n",
            "{'loss': 0.0016, 'grad_norm': 0.13461802899837494, 'learning_rate': 0.0008592592592592593, 'epoch': 0.68}\n",
            "{'loss': 0.0028, 'grad_norm': 0.029154621064662933, 'learning_rate': 0.0008518518518518519, 'epoch': 0.7}\n",
            "{'loss': 0.0018, 'grad_norm': 0.03870090842247009, 'learning_rate': 0.0008444444444444444, 'epoch': 0.72}\n",
            "{'loss': 0.0018, 'grad_norm': 0.007185724563896656, 'learning_rate': 0.0008370370370370371, 'epoch': 0.74}\n",
            "{'loss': 0.0033, 'grad_norm': 0.04181579872965813, 'learning_rate': 0.0008296296296296296, 'epoch': 0.76}\n",
            "{'loss': 0.002, 'grad_norm': 0.03194541111588478, 'learning_rate': 0.0008222222222222222, 'epoch': 0.78}\n",
            "{'loss': 0.0023, 'grad_norm': 0.019837364554405212, 'learning_rate': 0.0008148148148148148, 'epoch': 0.8}\n",
            "{'loss': 0.0009, 'grad_norm': 0.018556872382760048, 'learning_rate': 0.0008074074074074075, 'epoch': 0.82}\n",
            "{'loss': 0.0011, 'grad_norm': 0.012362946756184101, 'learning_rate': 0.0008, 'epoch': 0.84}\n",
            "{'loss': 0.001, 'grad_norm': 0.0062583331018686295, 'learning_rate': 0.0007925925925925927, 'epoch': 0.86}\n",
            "{'loss': 0.0011, 'grad_norm': 0.004335714969784021, 'learning_rate': 0.0007851851851851852, 'epoch': 0.88}\n",
            "{'loss': 0.0011, 'grad_norm': 0.018256597220897675, 'learning_rate': 0.0007777777777777778, 'epoch': 0.9}\n",
            "{'loss': 0.0007, 'grad_norm': 0.0017353255534544587, 'learning_rate': 0.0007703703703703704, 'epoch': 0.92}\n",
            "{'loss': 0.0005, 'grad_norm': 0.013207229785621166, 'learning_rate': 0.000762962962962963, 'epoch': 0.94}\n",
            "{'loss': 0.0007, 'grad_norm': 0.013569806702435017, 'learning_rate': 0.0007555555555555555, 'epoch': 0.96}\n",
            "{'loss': 0.0004, 'grad_norm': 0.006285662762820721, 'learning_rate': 0.0007481481481481482, 'epoch': 0.98}\n",
            "{'loss': 0.0005, 'grad_norm': 0.004501344170421362, 'learning_rate': 0.0007407407407407407, 'epoch': 1.0}\n",
            "{'loss': 0.0005, 'grad_norm': 0.008226685225963593, 'learning_rate': 0.0007333333333333333, 'epoch': 1.02}\n",
            "{'loss': 0.0008, 'grad_norm': 0.004299559630453587, 'learning_rate': 0.000725925925925926, 'epoch': 1.04}\n",
            "{'loss': 0.0006, 'grad_norm': 0.0020895509514957666, 'learning_rate': 0.0007185185185185186, 'epoch': 1.06}\n",
            "{'loss': 0.0005, 'grad_norm': 0.06347628682851791, 'learning_rate': 0.0007111111111111111, 'epoch': 1.08}\n",
            "{'loss': 0.0005, 'grad_norm': 0.019858313724398613, 'learning_rate': 0.0007037037037037038, 'epoch': 1.1}\n",
            "{'loss': 0.001, 'grad_norm': 0.0621504969894886, 'learning_rate': 0.0006962962962962963, 'epoch': 1.12}\n",
            "{'loss': 0.0003, 'grad_norm': 0.0020457454957067966, 'learning_rate': 0.000688888888888889, 'epoch': 1.14}\n",
            "{'loss': 0.0003, 'grad_norm': 0.002583224792033434, 'learning_rate': 0.0006814814814814815, 'epoch': 1.16}\n",
            "{'loss': 0.0004, 'grad_norm': 0.018765397369861603, 'learning_rate': 0.0006740740740740741, 'epoch': 1.18}\n",
            "{'loss': 0.0005, 'grad_norm': 0.0029901897069066763, 'learning_rate': 0.0006666666666666666, 'epoch': 1.2}\n",
            "{'loss': 0.0003, 'grad_norm': 0.003081322181969881, 'learning_rate': 0.0006592592592592592, 'epoch': 1.22}\n",
            "{'loss': 0.0004, 'grad_norm': 0.2533329427242279, 'learning_rate': 0.0006518518518518519, 'epoch': 1.24}\n",
            "{'loss': 0.0006, 'grad_norm': 0.00450506154447794, 'learning_rate': 0.0006444444444444444, 'epoch': 1.26}\n",
            "{'loss': 0.0016, 'grad_norm': 0.08768005669116974, 'learning_rate': 0.0006370370370370371, 'epoch': 1.28}\n",
            "{'loss': 0.0011, 'grad_norm': 0.07703684270381927, 'learning_rate': 0.0006296296296296296, 'epoch': 1.3}\n",
            "{'loss': 0.0009, 'grad_norm': 0.040344223380088806, 'learning_rate': 0.0006222222222222223, 'epoch': 1.32}\n",
            "{'loss': 0.0011, 'grad_norm': 0.0067032864317297935, 'learning_rate': 0.0006148148148148148, 'epoch': 1.34}\n",
            "{'loss': 0.0064, 'grad_norm': 0.18324004113674164, 'learning_rate': 0.0006074074074074074, 'epoch': 1.36}\n",
            "{'loss': 0.0046, 'grad_norm': 0.10339511185884476, 'learning_rate': 0.0006, 'epoch': 1.38}\n",
            "{'loss': 0.004, 'grad_norm': 0.09969517588615417, 'learning_rate': 0.0005925925925925926, 'epoch': 1.4}\n",
            "{'loss': 0.0057, 'grad_norm': 0.029794368892908096, 'learning_rate': 0.0005851851851851851, 'epoch': 1.42}\n",
            "{'loss': 0.0038, 'grad_norm': 0.02339898981153965, 'learning_rate': 0.0005777777777777778, 'epoch': 1.44}\n",
            "{'loss': 0.0017, 'grad_norm': 0.041445549577474594, 'learning_rate': 0.0005703703703703704, 'epoch': 1.46}\n",
            "{'loss': 0.0012, 'grad_norm': 0.03680624067783356, 'learning_rate': 0.000562962962962963, 'epoch': 1.48}\n",
            "{'loss': 0.0006, 'grad_norm': 0.003083013230934739, 'learning_rate': 0.0005555555555555556, 'epoch': 1.5}\n",
            "{'loss': 0.0004, 'grad_norm': 0.019575947895646095, 'learning_rate': 0.0005481481481481482, 'epoch': 1.52}\n",
            "{'loss': 0.0012, 'grad_norm': 0.05299718677997589, 'learning_rate': 0.0005407407407407407, 'epoch': 1.54}\n",
            "{'loss': 0.0005, 'grad_norm': 0.0531802736222744, 'learning_rate': 0.0005333333333333334, 'epoch': 1.56}\n",
            "{'loss': 0.0008, 'grad_norm': 0.06632828712463379, 'learning_rate': 0.0005259259259259259, 'epoch': 1.58}\n",
            "{'loss': 0.0007, 'grad_norm': 0.02213052101433277, 'learning_rate': 0.0005185185185185185, 'epoch': 1.6}\n",
            "{'loss': 0.0006, 'grad_norm': 0.013696881942451, 'learning_rate': 0.0005111111111111111, 'epoch': 1.62}\n",
            "{'loss': 0.0008, 'grad_norm': 0.006256936118006706, 'learning_rate': 0.0005037037037037037, 'epoch': 1.64}\n",
            "{'loss': 0.0011, 'grad_norm': 0.003669136203825474, 'learning_rate': 0.0004962962962962963, 'epoch': 1.66}\n",
            "{'loss': 0.0008, 'grad_norm': 0.03369186073541641, 'learning_rate': 0.0004888888888888889, 'epoch': 1.68}\n",
            "{'loss': 0.0005, 'grad_norm': 0.004279933404177427, 'learning_rate': 0.00048148148148148144, 'epoch': 1.7}\n",
            "{'loss': 0.0003, 'grad_norm': 0.0011080325348302722, 'learning_rate': 0.0004740740740740741, 'epoch': 1.72}\n",
            "{'loss': 0.0004, 'grad_norm': 0.0022269755136221647, 'learning_rate': 0.00046666666666666666, 'epoch': 1.74}\n",
            "{'loss': 0.0002, 'grad_norm': 0.0012120932806283236, 'learning_rate': 0.00045925925925925925, 'epoch': 1.76}\n",
            "{'loss': 0.0004, 'grad_norm': 0.0030953933019191027, 'learning_rate': 0.00045185185185185183, 'epoch': 1.78}\n",
            "{'loss': 0.0004, 'grad_norm': 0.0017450513551011682, 'learning_rate': 0.0004444444444444444, 'epoch': 1.8}\n",
            "{'loss': 0.0006, 'grad_norm': 0.0294583048671484, 'learning_rate': 0.00043703703703703705, 'epoch': 1.82}\n",
            "{'loss': 0.0002, 'grad_norm': 0.003341818694025278, 'learning_rate': 0.00042962962962962963, 'epoch': 1.84}\n",
            "{'loss': 0.0002, 'grad_norm': 0.0007955636247061193, 'learning_rate': 0.0004222222222222222, 'epoch': 1.86}\n",
            "{'loss': 0.0003, 'grad_norm': 0.007221207022666931, 'learning_rate': 0.0004148148148148148, 'epoch': 1.88}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00043766931048594415, 'learning_rate': 0.0004074074074074074, 'epoch': 1.9}\n",
            "{'loss': 0.0001, 'grad_norm': 0.048661936074495316, 'learning_rate': 0.0004, 'epoch': 1.92}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0003871163062285632, 'learning_rate': 0.0003925925925925926, 'epoch': 1.94}\n",
            "{'loss': 0.0001, 'grad_norm': 0.009438757784664631, 'learning_rate': 0.0003851851851851852, 'epoch': 1.96}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000853697070851922, 'learning_rate': 0.00037777777777777777, 'epoch': 1.98}\n",
            "{'loss': 0.0001, 'grad_norm': 0.001578037510626018, 'learning_rate': 0.00037037037037037035, 'epoch': 2.0}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000539918546564877, 'learning_rate': 0.000362962962962963, 'epoch': 2.02}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0004863633366767317, 'learning_rate': 0.00035555555555555557, 'epoch': 2.04}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0004653005162253976, 'learning_rate': 0.00034814814814814816, 'epoch': 2.06}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0007239102269522846, 'learning_rate': 0.00034074074074074074, 'epoch': 2.08}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00028726572054438293, 'learning_rate': 0.0003333333333333333, 'epoch': 2.1}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00041091733146458864, 'learning_rate': 0.00032592592592592596, 'epoch': 2.12}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00030926571344025433, 'learning_rate': 0.00031851851851851854, 'epoch': 2.14}\n",
            "{'loss': 0.0001, 'grad_norm': 0.038344018161296844, 'learning_rate': 0.0003111111111111111, 'epoch': 2.16}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0005626111524179578, 'learning_rate': 0.0003037037037037037, 'epoch': 2.18}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000564668676815927, 'learning_rate': 0.0002962962962962963, 'epoch': 2.2}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0024126458447426558, 'learning_rate': 0.0002888888888888889, 'epoch': 2.22}\n",
            "{'loss': 0.0004, 'grad_norm': 0.0002513570652808994, 'learning_rate': 0.0002814814814814815, 'epoch': 2.24}\n",
            "{'loss': 0.0003, 'grad_norm': 0.0033738247584551573, 'learning_rate': 0.0002740740740740741, 'epoch': 2.26}\n",
            "{'loss': 0.0002, 'grad_norm': 0.0004618332313839346, 'learning_rate': 0.0002666666666666667, 'epoch': 2.28}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000638923782389611, 'learning_rate': 0.00025925925925925926, 'epoch': 2.3}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0010443766368553042, 'learning_rate': 0.00025185185185185185, 'epoch': 2.32}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000653828086797148, 'learning_rate': 0.00024444444444444443, 'epoch': 2.34}\n",
            "{'loss': 0.0001, 'grad_norm': 0.000446058576926589, 'learning_rate': 0.00023703703703703704, 'epoch': 2.36}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0004256730026099831, 'learning_rate': 0.00022962962962962962, 'epoch': 2.38}\n",
            "{'loss': 0.0003, 'grad_norm': 0.0002877542865462601, 'learning_rate': 0.0002222222222222222, 'epoch': 2.4}\n",
            "{'loss': 0.0002, 'grad_norm': 0.0015410041669383645, 'learning_rate': 0.00021481481481481482, 'epoch': 2.42}\n",
            "{'loss': 0.0001, 'grad_norm': 0.010488891042768955, 'learning_rate': 0.0002074074074074074, 'epoch': 2.44}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0003385386080481112, 'learning_rate': 0.0002, 'epoch': 2.46}\n",
            "{'loss': 0.0003, 'grad_norm': 0.00028682645643129945, 'learning_rate': 0.0001925925925925926, 'epoch': 2.48}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00036234332947060466, 'learning_rate': 0.00018518518518518518, 'epoch': 2.5}\n",
            "{'loss': 0.0001, 'grad_norm': 0.018293578177690506, 'learning_rate': 0.00017777777777777779, 'epoch': 2.52}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0006206940161064267, 'learning_rate': 0.00017037037037037037, 'epoch': 2.54}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0005618719151243567, 'learning_rate': 0.00016296296296296298, 'epoch': 2.56}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0010777633870020509, 'learning_rate': 0.00015555555555555556, 'epoch': 2.58}\n",
            "{'loss': 0.0, 'grad_norm': 0.0006194724119268358, 'learning_rate': 0.00014814814814814815, 'epoch': 2.6}\n",
            "{'loss': 0.0, 'grad_norm': 0.0013263740111142397, 'learning_rate': 0.00014074074074074076, 'epoch': 2.62}\n",
            "{'loss': 0.0001, 'grad_norm': 0.00030385531135834754, 'learning_rate': 0.00013333333333333334, 'epoch': 2.64}\n",
            "{'loss': 0.0001, 'grad_norm': 0.0002790051221381873, 'learning_rate': 0.00012592592592592592, 'epoch': 2.66}\n",
            "{'loss': 0.0, 'grad_norm': 0.00024856632808223367, 'learning_rate': 0.00011851851851851852, 'epoch': 2.68}\n",
            "{'loss': 0.0, 'grad_norm': 0.00026708049699664116, 'learning_rate': 0.0001111111111111111, 'epoch': 2.7}\n",
            "{'loss': 0.0, 'grad_norm': 0.0002879594103433192, 'learning_rate': 0.0001037037037037037, 'epoch': 2.72}\n",
            "{'loss': 0.0, 'grad_norm': 0.00028562889201566577, 'learning_rate': 9.62962962962963e-05, 'epoch': 2.74}\n",
            "{'loss': 0.0, 'grad_norm': 0.00029266151250340044, 'learning_rate': 8.888888888888889e-05, 'epoch': 2.76}\n",
            "{'loss': 0.0, 'grad_norm': 0.0006913613178767264, 'learning_rate': 8.148148148148149e-05, 'epoch': 2.78}\n",
            "{'loss': 0.0, 'grad_norm': 0.0003764200664591044, 'learning_rate': 7.407407407407407e-05, 'epoch': 2.8}\n",
            "{'loss': 0.0, 'grad_norm': 0.0003662734816316515, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.82}\n",
            "{'loss': 0.0, 'grad_norm': 0.0002752956061158329, 'learning_rate': 5.925925925925926e-05, 'epoch': 2.84}\n",
            "{'loss': 0.0, 'grad_norm': 0.00029611654463224113, 'learning_rate': 5.185185185185185e-05, 'epoch': 2.86}\n",
            "{'loss': 0.0, 'grad_norm': 0.00029102759435772896, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.88}\n",
            "{'loss': 0.0, 'grad_norm': 0.00040096338489092886, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.9}\n",
            "{'loss': 0.0, 'grad_norm': 0.00023324570793192834, 'learning_rate': 2.962962962962963e-05, 'epoch': 2.92}\n",
            "{'loss': 0.0, 'grad_norm': 0.00030439303372986615, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.94}\n",
            "{'loss': 0.0, 'grad_norm': 0.00020668920478783548, 'learning_rate': 1.4814814814814815e-05, 'epoch': 2.96}\n",
            "{'loss': 0.0, 'grad_norm': 0.0007415892323479056, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.98}\n",
            "{'loss': 0.0, 'grad_norm': 0.0002353612071601674, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 3736.4013, 'train_samples_per_second': 6.423, 'train_steps_per_second': 0.401, 'train_loss': 0.08834487522183918, 'epoch': 3.0}\n",
            "100% 1500/1500 [1:02:16<00:00,  2.49s/it]\n",
            "[2025-02-16 23:27:01,717][oumi][rank0][pid:11003][MainThread][INFO]][train.py:320] Training is Complete.\n",
            "[2025-02-16 23:27:01,722][oumi][rank0][pid:11003][MainThread][INFO]][device_utils.py:283] GPU Metrics After Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=11237.0, temperature=39, fan_speed=None, fan_speeds=None, power_usage_watts=107.377, power_limit_watts=400.0, gpu_utilization=28, memory_utilization=8, performance_state=0, clock_speed_graphics=1410, clock_speed_sm=1410, clock_speed_memory=1215).\n",
            "[2025-02-16 23:27:01,722][oumi][rank0][pid:11003][MainThread][INFO]][torch_utils.py:117] Peak GPU memory usage: 4.78 GB\n",
            "[2025-02-16 23:27:01,722][oumi][rank0][pid:11003][MainThread][INFO]][train.py:327] Saving final state...\n",
            "[2025-02-16 23:27:01,726][oumi][rank0][pid:11003][MainThread][INFO]][train.py:332] Saving final model...\n",
            "[2025-02-16 23:27:02,168][oumi][rank0][pid:11003][MainThread][INFO]][hf_trainer.py:102] Model has been saved at finetuning_tutorial/output\n",
            "[2025-02-16 23:27:02,168][oumi][rank0][pid:11003][MainThread][INFO]][train.py:339] \n",
            "\n",
            "Â» We're always looking for feedback. What's one thing we can improve? https://oumi.ai/feedback\n"
          ]
        }
      ],
      "source": [
        "!oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQ1lbgkyd6v"
      },
      "source": [
        "### MULTI-GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "4j8a_SXGyd6v"
      },
      "outputs": [],
      "source": [
        "!oumi distributed torchrun -m oumi train -c \"$tutorial_dir/train.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpXyY5phyd6v"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "GP9Vf9Deyd6w"
      },
      "source": [
        "\n",
        "As an example, let's create an evaluation configuration file!\n",
        "\n",
        "**Note:** Since we've finetuned our model to produce thoughts before answering, it's very likely to do worse on most evals out-of-the-box.\n",
        "\n",
        "Many evals do not allow models to decode and thus don't take advantage of things like inference-time reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aRuqu7D8yd6w",
        "outputId": "e1071404-9e42-4c86-fa0e-8c292d27833d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting finetuning_tutorial/eval.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/eval.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"finetuning_tutorial/output\"\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "tasks:\n",
        "  - evaluation_platform: lm_harness\n",
        "    task_name: mmlu_college_computer_science\n",
        "\n",
        "output_dir: \"finetuning_tutorial/output/evaluation\"\n",
        "generation:\n",
        "  batch_size: null # This will let LM HARNESS find the maximum possible batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DuAq74XLyd6w",
        "outputId": "c3311467-c533-4075-84d7-e4574bd466c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "@                 @\n",
            "@   @@@@@  @  @   @\n",
            "@   @   @  @  @   @\n",
            "@   @@@@@  @@@@   @\n",
            "@                 @\n",
            "@   @@@@@@@   @   @\n",
            "@   @  @  @   @   @\n",
            "@   @  @  @   @   @\n",
            "@                 @\n",
            "@@@@@@@@@@@@@@@@@@@\n",
            "\n",
            "[2025-02-16 23:27:30,659][oumi][rank0][pid:26526][MainThread][INFO]][model_params.py:225] Found LoRA adapter at finetuning_tutorial/output, setting `adapter_model` to `model_name`.\n",
            "[2025-02-16 23:27:30,659][oumi][rank0][pid:26526][MainThread][INFO]][model_params.py:242] Setting `model_name` to HuggingFaceTB/SmolLM2-1.7B-Instruct found in adapter config.\n",
            "2025-02-16 23:27:32.600112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739748452.622490   26526 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739748452.629416   26526 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-02-16 23:27:36,785][oumi][rank0][pid:26526][MainThread][INFO]][lm_harness.py:81] Loading adapter for eval: finetuning_tutorial/output\n",
            "[2025-02-16 23:27:36,890][oumi][rank0][pid:26526][MainThread][INFO]][lm_harness.py:110] Starting evaluation...\n",
            "[2025-02-16 23:27:36,890][oumi][rank0][pid:26526][MainThread][INFO]][lm_harness.py:111] \tLM Harness `model_params`:\n",
            "{'device_map': 'auto',\n",
            " 'dtype': torch.bfloat16,\n",
            " 'parallelize': False,\n",
            " 'peft': 'finetuning_tutorial/output',\n",
            " 'pretrained': 'HuggingFaceTB/SmolLM2-1.7B-Instruct',\n",
            " 'trust_remote_code': False}\n",
            "[2025-02-16 23:27:36,890][oumi][rank0][pid:26526][MainThread][INFO]][lm_harness.py:112] \tLM Harness `task_params`:\n",
            "LMHarnessTaskParams(evaluation_platform='lm_harness',\n",
            "                    task_name='mmlu_college_computer_science',\n",
            "                    num_samples=None,\n",
            "                    eval_kwargs={},\n",
            "                    num_fewshot=None)\n",
            "INFO:lm-eval:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm-eval:Initializing hf model, with arguments: {'pretrained': 'HuggingFaceTB/SmolLM2-1.7B-Instruct', 'trust_remote_code': False, 'parallelize': False, 'dtype': torch.bfloat16, 'device_map': 'auto', 'peft': 'finetuning_tutorial/output'}\n",
            "INFO:lm-eval:Using device 'cuda:0'\n",
            "INFO:lm-eval:Model parallel was set to False.\n",
            "INFO:lm-eval:Building contexts for mmlu_college_computer_science on rank 0...\n",
            "100% 100/100 [00:00<00:00, 703.34it/s]\n",
            "INFO:lm-eval:Running loglikelihood requests\n",
            "Running loglikelihood requests:   0% 0/400 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests: 100% 400/400 [00:04<00:00, 83.00it/s]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "[2025-02-16 23:27:54,260][oumi][rank0][pid:26526][MainThread][INFO]][lm_harness.py:132] mmlu_college_computer_science's metric dict is {'acc,none': 0.36,\n",
            " 'acc_stderr,none': 0.04824181513244218,\n",
            " 'alias': 'college_computer_science'}\n"
          ]
        }
      ],
      "source": [
        "!oumi evaluate -c \"$tutorial_dir/eval.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "VC9C6K_Byd6w"
      },
      "source": [
        "## Use the Fine-tuned Model\n",
        "\n",
        "Once we're happy with the results, we can serve the fine-tuned model for interactive inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r6sft71lyd6w",
        "outputId": "16428706-6625-4f82-a1d2-4ed6cf7335b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting finetuning_tutorial/trained_infer.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $tutorial_dir/trained_infer.yaml\n",
        "\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "  adapter_model: \"finetuning_tutorial/output\"\n",
        "  trust_remote_code: true\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "\n",
        "generation:\n",
        "  max_new_tokens: 2048\n",
        "  batch_size: 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5S6Y33WSyd6w",
        "outputId": "4de199aa-4479-4882-8aff-0e7e7659c19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-02-16 23:28:24,296][oumi][rank0][pid:7702][MainThread][WARNING]][infer.py:19] No inference engine specified. Using the default 'native' engine.\n",
            "[2025-02-16 23:28:24,297][oumi][rank0][pid:7702][MainThread][INFO]][models.py:185] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-02-16 23:28:24,298][oumi][rank0][pid:7702][MainThread][INFO]][models.py:255] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n",
            "[2025-02-16 23:28:26,023][oumi][rank0][pid:7702][MainThread][INFO]][models.py:236] Loading PEFT adapter from: finetuning_tutorial/output ...\n",
            "[2025-02-16 23:28:26,682][oumi][rank0][pid:7702][MainThread][INFO]][native_text_inference_engine.py:111] Setting EOS token id to `2`\n",
            "conversation_id=None messages=[USER: What do I need to do if my SAP Fiori app deployment is not working?, ASSISTANT: Please follow the steps below. Also, replace the following variables in the curl command templates in steps 1 and 3.\n",
            "\n",
            "$DESTINATION: Use the destination name in BAS environment\n",
            "$SET_COOKIE_ARBE: Replace with ARBE value that can be found in Set-Cookie header in output.txt (From Step 2)\n",
            "$CSRF: Replace with response header X-Csrf-Token that can be found in output.txt (From Step 2).\n",
            "$PACKAGE_NAME: Use a valid package name in your system (if name contains slash \"/\", then encode it first. You can use https://www.urlencoder.org/)\n",
            "$APP_NAME: Use a valid app name in your system (if name contains slash \"/\", then encode it first. You can use https://www.urlencoder.org/)\n",
            "\n",
            "1. Run the ADT schema discovery query. Since the ADT schema is a very large XML file, we need to export the http response in an output file (e.g. output.txt)\n",
            "    curl -X GET -H X-Csrf-Token:Fetch $DESTINATION.dest/sap/bc/adt/discovery > output.txt\n",
            "\n",
            "2. Verify the content in output.txt file. We will need to use set-cookie ARBE and response header X-Csrf-Token in the next step.\n",
            "\n",
            "(See sample image below)\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "Content-Type: application/atoms+xml\n",
            "Date: Tue, 10 Jul 2023 16:58:24 GMT\n",
            "Sap-Cache-Control: *\n",
            "Sap-Log-System: \n",
            "Sap-Seq-Request: \n",
            "Sap-Referrer: 735699.000000\n",
            "Server: true\n",
            "\n",
            "Set-Cookie: ARBE=9d240287c236c6816718aab935c64ef34f3127cb1f7345b7241665484e70c429be97443c32126608185f89...\n",
            "X-Csrf-Token: hv8stCrYvvlxw6K\n",
            "X-Proxy-Authorization: \n",
            "X-Envoy-Upstream-Service-Time: 229\n",
            "X-Request-Id: e6b5e856-3440-4789-827b-6907134a3343\n",
            "Transfer-Encoding: chunked\n",
            "\n",
            "// followed by the large ADT schema XML string\n",
            "\n",
            "3. Execute the curl command for listing transport ADT service. The result (transport list) is exported in file output2.txt as XML string.\n",
            "\n",
            "curl -X POST \\\n",
            " -H \"ARBE=$SET_COOKIE_ARBE\" \\\n",
            " -H \"X-Csrf-Token:$CSRF\" \\\n",
            " -H \"Accept: application/vnd.sap.as+xml; dataname=com.sap.adt.transport.service.checkData\" \\\n",
            " -H \"Content-Type: application/vnd.sap.as+xml; charset=UTF-8; datasource= com.sap.adt.transport.service.checkData\" \\\n",
            " -d \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><asx:abap xmlns:asx=\\\"http://www.sap.com/abapxm\\\" version=\\\"1\\\"><asx:values><DATA><PGMID/><OBJECT/><...\"></DATA></asx:values></asx:abap>\" \\\n",
            " $DESTINATION.dest/sap/bc/adt/transportcatch > output2.txt\n",
            "\n",
            "Guided Answers URL: https://ga.support.sap.com/gc/behavior/index.html#/tree/3046/actions/459959:45996:45999:46000:57266] metadata={}\n"
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import InferenceConfig\n",
        "from oumi.infer import infer\n",
        "\n",
        "config = InferenceConfig.from_yaml(str(Path(tutorial_dir) / \"trained_infer.yaml\"))\n",
        "\n",
        "input_text = (\n",
        "    \"What do I need to do if my SAP Fiori app deployment is not working?\"\n",
        ")\n",
        "\n",
        "results = infer(config=config, inputs=[input_text])\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"finetuning_tutorial/output\")\n",
        "model.push_to_hub(\"Unseen1980/fiori-tools-customer-support\", token = \"\")"
      ],
      "metadata": {
        "id": "BATDdYev60pZ",
        "outputId": "baea0f7a-1eae-4d79-8b70-addd51bab8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "33db1c9a070046f18d0ce46cf3c240f1",
            "612b3c8363fe41c0b21cd7be566ced23",
            "e719a05454d243a6860abc16b8eb8d48",
            "6995df35cb2041998f53dc6e4a99d095",
            "f1ccdf0e16524cbe8b99edef52fd60d2",
            "09ff009c5de94b0588304d620c00e26a",
            "0823384eddb8431fbb817805a4b8c1c2",
            "fb6751e127494a1cb01547b94053f013",
            "394e4b927b064047a95f7ee0334f71b2",
            "ff6ed5e771b34cd6a732a7f1b65dd507",
            "fee7cd08d5a144d2b2729dbf25e407d2",
            "eaa0a51db1504ed99d236ed85d40008f",
            "33ff380301df40c39c0abed353a9729a",
            "02ab7a91f59446c5917e78f6b85e4621",
            "4056cd321e2d41e7be7956f97edc7b86",
            "349b70ab79b5473083b10a4374aec8c7",
            "10201236298f410a9f182757052674b3",
            "792645a9268a427b9898e7fd7da78c94",
            "93276e02b14c4513b63efc55bf7e977d",
            "8f3db95afaaf4d528f13cf31a60047e8",
            "3ff3fafcfbef4b949783071b5226a8b5",
            "18bbf7a5f77944e9b7f510c32bf3f79f"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.45.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading adapter weights from finetuning_tutorial/output led to unexpected keys not found in the model:  ['model.layers.0.mlp.down_proj.lora_A.default.weight', 'model.layers.0.mlp.down_proj.lora_B.default.weight', 'model.layers.0.mlp.gate_proj.lora_A.default.weight', 'model.layers.0.mlp.gate_proj.lora_B.default.weight', 'model.layers.0.mlp.up_proj.lora_A.default.weight', 'model.layers.0.mlp.up_proj.lora_B.default.weight', 'model.layers.0.self_attn.k_proj.lora_A.default.weight', 'model.layers.0.self_attn.k_proj.lora_B.default.weight', 'model.layers.0.self_attn.o_proj.lora_A.default.weight', 'model.layers.0.self_attn.o_proj.lora_B.default.weight', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.mlp.down_proj.lora_A.default.weight', 'model.layers.1.mlp.down_proj.lora_B.default.weight', 'model.layers.1.mlp.gate_proj.lora_A.default.weight', 'model.layers.1.mlp.gate_proj.lora_B.default.weight', 'model.layers.1.mlp.up_proj.lora_A.default.weight', 'model.layers.1.mlp.up_proj.lora_B.default.weight', 'model.layers.1.self_attn.k_proj.lora_A.default.weight', 'model.layers.1.self_attn.k_proj.lora_B.default.weight', 'model.layers.1.self_attn.o_proj.lora_A.default.weight', 'model.layers.1.self_attn.o_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.mlp.down_proj.lora_A.default.weight', 'model.layers.10.mlp.down_proj.lora_B.default.weight', 'model.layers.10.mlp.gate_proj.lora_A.default.weight', 'model.layers.10.mlp.gate_proj.lora_B.default.weight', 'model.layers.10.mlp.up_proj.lora_A.default.weight', 'model.layers.10.mlp.up_proj.lora_B.default.weight', 'model.layers.10.self_attn.k_proj.lora_A.default.weight', 'model.layers.10.self_attn.k_proj.lora_B.default.weight', 'model.layers.10.self_attn.o_proj.lora_A.default.weight', 'model.layers.10.self_attn.o_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.mlp.down_proj.lora_A.default.weight', 'model.layers.11.mlp.down_proj.lora_B.default.weight', 'model.layers.11.mlp.gate_proj.lora_A.default.weight', 'model.layers.11.mlp.gate_proj.lora_B.default.weight', 'model.layers.11.mlp.up_proj.lora_A.default.weight', 'model.layers.11.mlp.up_proj.lora_B.default.weight', 'model.layers.11.self_attn.k_proj.lora_A.default.weight', 'model.layers.11.self_attn.k_proj.lora_B.default.weight', 'model.layers.11.self_attn.o_proj.lora_A.default.weight', 'model.layers.11.self_attn.o_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.mlp.down_proj.lora_A.default.weight', 'model.layers.12.mlp.down_proj.lora_B.default.weight', 'model.layers.12.mlp.gate_proj.lora_A.default.weight', 'model.layers.12.mlp.gate_proj.lora_B.default.weight', 'model.layers.12.mlp.up_proj.lora_A.default.weight', 'model.layers.12.mlp.up_proj.lora_B.default.weight', 'model.layers.12.self_attn.k_proj.lora_A.default.weight', 'model.layers.12.self_attn.k_proj.lora_B.default.weight', 'model.layers.12.self_attn.o_proj.lora_A.default.weight', 'model.layers.12.self_attn.o_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.mlp.down_proj.lora_A.default.weight', 'model.layers.13.mlp.down_proj.lora_B.default.weight', 'model.layers.13.mlp.gate_proj.lora_A.default.weight', 'model.layers.13.mlp.gate_proj.lora_B.default.weight', 'model.layers.13.mlp.up_proj.lora_A.default.weight', 'model.layers.13.mlp.up_proj.lora_B.default.weight', 'model.layers.13.self_attn.k_proj.lora_A.default.weight', 'model.layers.13.self_attn.k_proj.lora_B.default.weight', 'model.layers.13.self_attn.o_proj.lora_A.default.weight', 'model.layers.13.self_attn.o_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.mlp.down_proj.lora_A.default.weight', 'model.layers.14.mlp.down_proj.lora_B.default.weight', 'model.layers.14.mlp.gate_proj.lora_A.default.weight', 'model.layers.14.mlp.gate_proj.lora_B.default.weight', 'model.layers.14.mlp.up_proj.lora_A.default.weight', 'model.layers.14.mlp.up_proj.lora_B.default.weight', 'model.layers.14.self_attn.k_proj.lora_A.default.weight', 'model.layers.14.self_attn.k_proj.lora_B.default.weight', 'model.layers.14.self_attn.o_proj.lora_A.default.weight', 'model.layers.14.self_attn.o_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.mlp.down_proj.lora_A.default.weight', 'model.layers.15.mlp.down_proj.lora_B.default.weight', 'model.layers.15.mlp.gate_proj.lora_A.default.weight', 'model.layers.15.mlp.gate_proj.lora_B.default.weight', 'model.layers.15.mlp.up_proj.lora_A.default.weight', 'model.layers.15.mlp.up_proj.lora_B.default.weight', 'model.layers.15.self_attn.k_proj.lora_A.default.weight', 'model.layers.15.self_attn.k_proj.lora_B.default.weight', 'model.layers.15.self_attn.o_proj.lora_A.default.weight', 'model.layers.15.self_attn.o_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.mlp.down_proj.lora_A.default.weight', 'model.layers.16.mlp.down_proj.lora_B.default.weight', 'model.layers.16.mlp.gate_proj.lora_A.default.weight', 'model.layers.16.mlp.gate_proj.lora_B.default.weight', 'model.layers.16.mlp.up_proj.lora_A.default.weight', 'model.layers.16.mlp.up_proj.lora_B.default.weight', 'model.layers.16.self_attn.k_proj.lora_A.default.weight', 'model.layers.16.self_attn.k_proj.lora_B.default.weight', 'model.layers.16.self_attn.o_proj.lora_A.default.weight', 'model.layers.16.self_attn.o_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.mlp.down_proj.lora_A.default.weight', 'model.layers.17.mlp.down_proj.lora_B.default.weight', 'model.layers.17.mlp.gate_proj.lora_A.default.weight', 'model.layers.17.mlp.gate_proj.lora_B.default.weight', 'model.layers.17.mlp.up_proj.lora_A.default.weight', 'model.layers.17.mlp.up_proj.lora_B.default.weight', 'model.layers.17.self_attn.k_proj.lora_A.default.weight', 'model.layers.17.self_attn.k_proj.lora_B.default.weight', 'model.layers.17.self_attn.o_proj.lora_A.default.weight', 'model.layers.17.self_attn.o_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.mlp.down_proj.lora_A.default.weight', 'model.layers.18.mlp.down_proj.lora_B.default.weight', 'model.layers.18.mlp.gate_proj.lora_A.default.weight', 'model.layers.18.mlp.gate_proj.lora_B.default.weight', 'model.layers.18.mlp.up_proj.lora_A.default.weight', 'model.layers.18.mlp.up_proj.lora_B.default.weight', 'model.layers.18.self_attn.k_proj.lora_A.default.weight', 'model.layers.18.self_attn.k_proj.lora_B.default.weight', 'model.layers.18.self_attn.o_proj.lora_A.default.weight', 'model.layers.18.self_attn.o_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.mlp.down_proj.lora_A.default.weight', 'model.layers.19.mlp.down_proj.lora_B.default.weight', 'model.layers.19.mlp.gate_proj.lora_A.default.weight', 'model.layers.19.mlp.gate_proj.lora_B.default.weight', 'model.layers.19.mlp.up_proj.lora_A.default.weight', 'model.layers.19.mlp.up_proj.lora_B.default.weight', 'model.layers.19.self_attn.k_proj.lora_A.default.weight', 'model.layers.19.self_attn.k_proj.lora_B.default.weight', 'model.layers.19.self_attn.o_proj.lora_A.default.weight', 'model.layers.19.self_attn.o_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.mlp.down_proj.lora_A.default.weight', 'model.layers.2.mlp.down_proj.lora_B.default.weight', 'model.layers.2.mlp.gate_proj.lora_A.default.weight', 'model.layers.2.mlp.gate_proj.lora_B.default.weight', 'model.layers.2.mlp.up_proj.lora_A.default.weight', 'model.layers.2.mlp.up_proj.lora_B.default.weight', 'model.layers.2.self_attn.k_proj.lora_A.default.weight', 'model.layers.2.self_attn.k_proj.lora_B.default.weight', 'model.layers.2.self_attn.o_proj.lora_A.default.weight', 'model.layers.2.self_attn.o_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.mlp.down_proj.lora_A.default.weight', 'model.layers.20.mlp.down_proj.lora_B.default.weight', 'model.layers.20.mlp.gate_proj.lora_A.default.weight', 'model.layers.20.mlp.gate_proj.lora_B.default.weight', 'model.layers.20.mlp.up_proj.lora_A.default.weight', 'model.layers.20.mlp.up_proj.lora_B.default.weight', 'model.layers.20.self_attn.k_proj.lora_A.default.weight', 'model.layers.20.self_attn.k_proj.lora_B.default.weight', 'model.layers.20.self_attn.o_proj.lora_A.default.weight', 'model.layers.20.self_attn.o_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.mlp.down_proj.lora_A.default.weight', 'model.layers.21.mlp.down_proj.lora_B.default.weight', 'model.layers.21.mlp.gate_proj.lora_A.default.weight', 'model.layers.21.mlp.gate_proj.lora_B.default.weight', 'model.layers.21.mlp.up_proj.lora_A.default.weight', 'model.layers.21.mlp.up_proj.lora_B.default.weight', 'model.layers.21.self_attn.k_proj.lora_A.default.weight', 'model.layers.21.self_attn.k_proj.lora_B.default.weight', 'model.layers.21.self_attn.o_proj.lora_A.default.weight', 'model.layers.21.self_attn.o_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.mlp.down_proj.lora_A.default.weight', 'model.layers.22.mlp.down_proj.lora_B.default.weight', 'model.layers.22.mlp.gate_proj.lora_A.default.weight', 'model.layers.22.mlp.gate_proj.lora_B.default.weight', 'model.layers.22.mlp.up_proj.lora_A.default.weight', 'model.layers.22.mlp.up_proj.lora_B.default.weight', 'model.layers.22.self_attn.k_proj.lora_A.default.weight', 'model.layers.22.self_attn.k_proj.lora_B.default.weight', 'model.layers.22.self_attn.o_proj.lora_A.default.weight', 'model.layers.22.self_attn.o_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.mlp.down_proj.lora_A.default.weight', 'model.layers.23.mlp.down_proj.lora_B.default.weight', 'model.layers.23.mlp.gate_proj.lora_A.default.weight', 'model.layers.23.mlp.gate_proj.lora_B.default.weight', 'model.layers.23.mlp.up_proj.lora_A.default.weight', 'model.layers.23.mlp.up_proj.lora_B.default.weight', 'model.layers.23.self_attn.k_proj.lora_A.default.weight', 'model.layers.23.self_attn.k_proj.lora_B.default.weight', 'model.layers.23.self_attn.o_proj.lora_A.default.weight', 'model.layers.23.self_attn.o_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.mlp.down_proj.lora_A.default.weight', 'model.layers.3.mlp.down_proj.lora_B.default.weight', 'model.layers.3.mlp.gate_proj.lora_A.default.weight', 'model.layers.3.mlp.gate_proj.lora_B.default.weight', 'model.layers.3.mlp.up_proj.lora_A.default.weight', 'model.layers.3.mlp.up_proj.lora_B.default.weight', 'model.layers.3.self_attn.k_proj.lora_A.default.weight', 'model.layers.3.self_attn.k_proj.lora_B.default.weight', 'model.layers.3.self_attn.o_proj.lora_A.default.weight', 'model.layers.3.self_attn.o_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.mlp.down_proj.lora_A.default.weight', 'model.layers.4.mlp.down_proj.lora_B.default.weight', 'model.layers.4.mlp.gate_proj.lora_A.default.weight', 'model.layers.4.mlp.gate_proj.lora_B.default.weight', 'model.layers.4.mlp.up_proj.lora_A.default.weight', 'model.layers.4.mlp.up_proj.lora_B.default.weight', 'model.layers.4.self_attn.k_proj.lora_A.default.weight', 'model.layers.4.self_attn.k_proj.lora_B.default.weight', 'model.layers.4.self_attn.o_proj.lora_A.default.weight', 'model.layers.4.self_attn.o_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.mlp.down_proj.lora_A.default.weight', 'model.layers.5.mlp.down_proj.lora_B.default.weight', 'model.layers.5.mlp.gate_proj.lora_A.default.weight', 'model.layers.5.mlp.gate_proj.lora_B.default.weight', 'model.layers.5.mlp.up_proj.lora_A.default.weight', 'model.layers.5.mlp.up_proj.lora_B.default.weight', 'model.layers.5.self_attn.k_proj.lora_A.default.weight', 'model.layers.5.self_attn.k_proj.lora_B.default.weight', 'model.layers.5.self_attn.o_proj.lora_A.default.weight', 'model.layers.5.self_attn.o_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.mlp.down_proj.lora_A.default.weight', 'model.layers.6.mlp.down_proj.lora_B.default.weight', 'model.layers.6.mlp.gate_proj.lora_A.default.weight', 'model.layers.6.mlp.gate_proj.lora_B.default.weight', 'model.layers.6.mlp.up_proj.lora_A.default.weight', 'model.layers.6.mlp.up_proj.lora_B.default.weight', 'model.layers.6.self_attn.k_proj.lora_A.default.weight', 'model.layers.6.self_attn.k_proj.lora_B.default.weight', 'model.layers.6.self_attn.o_proj.lora_A.default.weight', 'model.layers.6.self_attn.o_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.mlp.down_proj.lora_A.default.weight', 'model.layers.7.mlp.down_proj.lora_B.default.weight', 'model.layers.7.mlp.gate_proj.lora_A.default.weight', 'model.layers.7.mlp.gate_proj.lora_B.default.weight', 'model.layers.7.mlp.up_proj.lora_A.default.weight', 'model.layers.7.mlp.up_proj.lora_B.default.weight', 'model.layers.7.self_attn.k_proj.lora_A.default.weight', 'model.layers.7.self_attn.k_proj.lora_B.default.weight', 'model.layers.7.self_attn.o_proj.lora_A.default.weight', 'model.layers.7.self_attn.o_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.mlp.down_proj.lora_A.default.weight', 'model.layers.8.mlp.down_proj.lora_B.default.weight', 'model.layers.8.mlp.gate_proj.lora_A.default.weight', 'model.layers.8.mlp.gate_proj.lora_B.default.weight', 'model.layers.8.mlp.up_proj.lora_A.default.weight', 'model.layers.8.mlp.up_proj.lora_B.default.weight', 'model.layers.8.self_attn.k_proj.lora_A.default.weight', 'model.layers.8.self_attn.k_proj.lora_B.default.weight', 'model.layers.8.self_attn.o_proj.lora_A.default.weight', 'model.layers.8.self_attn.o_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.mlp.down_proj.lora_A.default.weight', 'model.layers.9.mlp.down_proj.lora_B.default.weight', 'model.layers.9.mlp.gate_proj.lora_A.default.weight', 'model.layers.9.mlp.gate_proj.lora_B.default.weight', 'model.layers.9.mlp.up_proj.lora_A.default.weight', 'model.layers.9.mlp.up_proj.lora_B.default.weight', 'model.layers.9.self_attn.k_proj.lora_A.default.weight', 'model.layers.9.self_attn.k_proj.lora_B.default.weight', 'model.layers.9.self_attn.o_proj.lora_A.default.weight', 'model.layers.9.self_attn.o_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33db1c9a070046f18d0ce46cf3c240f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/72.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa0a51db1504ed99d236ed85d40008f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Unseen1980/fiori-tools-customer-support/commit/1a7b88a4aeb899d05737805523e436619fb80feb', commit_message='Upload model', commit_description='', oid='1a7b88a4aeb899d05737805523e436619fb80feb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Unseen1980/fiori-tools-customer-support', endpoint='https://huggingface.co', repo_type='model', repo_id='Unseen1980/fiori-tools-customer-support'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33db1c9a070046f18d0ce46cf3c240f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_612b3c8363fe41c0b21cd7be566ced23",
              "IPY_MODEL_e719a05454d243a6860abc16b8eb8d48",
              "IPY_MODEL_6995df35cb2041998f53dc6e4a99d095"
            ],
            "layout": "IPY_MODEL_f1ccdf0e16524cbe8b99edef52fd60d2"
          }
        },
        "612b3c8363fe41c0b21cd7be566ced23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09ff009c5de94b0588304d620c00e26a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0823384eddb8431fbb817805a4b8c1c2",
            "value": "README.md:â€‡100%"
          }
        },
        "e719a05454d243a6860abc16b8eb8d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb6751e127494a1cb01547b94053f013",
            "max": 5174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394e4b927b064047a95f7ee0334f71b2",
            "value": 5174
          }
        },
        "6995df35cb2041998f53dc6e4a99d095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff6ed5e771b34cd6a732a7f1b65dd507",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fee7cd08d5a144d2b2729dbf25e407d2",
            "value": "â€‡5.17k/5.17kâ€‡[00:00&lt;00:00,â€‡409kB/s]"
          }
        },
        "f1ccdf0e16524cbe8b99edef52fd60d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ff009c5de94b0588304d620c00e26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0823384eddb8431fbb817805a4b8c1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb6751e127494a1cb01547b94053f013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394e4b927b064047a95f7ee0334f71b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff6ed5e771b34cd6a732a7f1b65dd507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee7cd08d5a144d2b2729dbf25e407d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa0a51db1504ed99d236ed85d40008f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33ff380301df40c39c0abed353a9729a",
              "IPY_MODEL_02ab7a91f59446c5917e78f6b85e4621",
              "IPY_MODEL_4056cd321e2d41e7be7956f97edc7b86"
            ],
            "layout": "IPY_MODEL_349b70ab79b5473083b10a4374aec8c7"
          }
        },
        "33ff380301df40c39c0abed353a9729a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10201236298f410a9f182757052674b3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_792645a9268a427b9898e7fd7da78c94",
            "value": "adapter_model.safetensors:â€‡100%"
          }
        },
        "02ab7a91f59446c5917e78f6b85e4621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93276e02b14c4513b63efc55bf7e977d",
            "max": 72394360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f3db95afaaf4d528f13cf31a60047e8",
            "value": 72394360
          }
        },
        "4056cd321e2d41e7be7956f97edc7b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ff3fafcfbef4b949783071b5226a8b5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18bbf7a5f77944e9b7f510c32bf3f79f",
            "value": "â€‡72.4M/72.4Mâ€‡[00:02&lt;00:00,â€‡18.1MB/s]"
          }
        },
        "349b70ab79b5473083b10a4374aec8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10201236298f410a9f182757052674b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792645a9268a427b9898e7fd7da78c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93276e02b14c4513b63efc55bf7e977d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3db95afaaf4d528f13cf31a60047e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ff3fafcfbef4b949783071b5226a8b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18bbf7a5f77944e9b7f510c32bf3f79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}